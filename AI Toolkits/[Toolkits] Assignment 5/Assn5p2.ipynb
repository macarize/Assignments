{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset"
      ],
      "metadata": {
        "id": "oADkiNxj5uX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unFWSm4u2oCc",
        "outputId": "53736963-2080-4552-89db-1b59189ea582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-17 09:15:50--  https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/labels.txt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/labels.txt [following]\n",
            "--2021-12-17 09:15:51--  https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/labels.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 225000 (220K) [text/plain]\n",
            "Saving to: ‘labels.txt’\n",
            "\n",
            "labels.txt          100%[===================>] 219.73K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2021-12-17 09:15:51 (30.5 MB/s) - ‘labels.txt’ saved [225000/225000]\n",
            "\n",
            "--2021-12-17 09:15:51--  https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/reviews.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/reviews.txt [following]\n",
            "--2021-12-17 09:15:51--  https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/reviews.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33678267 (32M) [text/plain]\n",
            "Saving to: ‘reviews.txt’\n",
            "\n",
            "reviews.txt         100%[===================>]  32.12M   154MB/s    in 0.2s    \n",
            "\n",
            "2021-12-17 09:15:51 (154 MB/s) - ‘reviews.txt’ saved [33678267/33678267]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!wget -c https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/labels.txt\n",
        "!wget -c https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/reviews.txt\n",
        "!mv *.txt data/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# read data from text files\n",
        "with open('data/reviews.txt', 'r') as f:\n",
        "    reviews = f.read()\n",
        "with open('data/labels.txt', 'r') as f:\n",
        "    labels = f.read()"
      ],
      "metadata": {
        "id": "BUcYpos22673"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reviews[:1000])\n",
        "print()\n",
        "print(labels[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUohH6OE2_ul",
        "outputId": "37924122-7471-4011-fc87-903758aad049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
            "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
            "\n",
            "positive\n",
            "negative\n",
            "positive\n",
            "neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As data preprocessing, we remove all punctuation, then get all the text without the newlines and split it into individual words."
      ],
      "metadata": {
        "id": "fY8JnDZH3NfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "\n",
        "print(punctuation)\n",
        "\n",
        "# get rid of punctuation\n",
        "reviews = reviews.lower() # lowercase, standardize\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOBTs9l13Vi4",
        "outputId": "7336c2ea-04f9-4977-d4fd-3dc9d90f98d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split by new lines and spaces\n",
        "reviews_split = all_text.split('\\n')\n",
        "all_text = ' '.join(reviews_split)\n",
        "\n",
        "# create a list of words\n",
        "words = all_text.split()"
      ],
      "metadata": {
        "id": "W-8MgC8y3dC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the first 20 words"
      ],
      "metadata": {
        "id": "ch1jYbOB5_zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IGObaFV3jXO",
        "outputId": "9b5559f7-c526-4620-a76a-d56b3023e8f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell',\n",
              " 'high',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cartoon',\n",
              " 'comedy',\n",
              " 'it',\n",
              " 'ran',\n",
              " 'at',\n",
              " 'the',\n",
              " 'same',\n",
              " 'time',\n",
              " 'as',\n",
              " 'some',\n",
              " 'other',\n",
              " 'programs',\n",
              " 'about',\n",
              " 'school',\n",
              " 'life',\n",
              " 'such']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to encoding the text into integers to be used as input to your neural network model. To pad 0s to make each sequence length the same, those integers start from 1, not 0. You may build a dictionary to map words to integers."
      ],
      "metadata": {
        "id": "W_1Mf8Bu4P_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feel free to use this import\n",
        "from collections import Counter\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab,1)}\n",
        "\n",
        "## use the dict to tokenize each review in reviews_split\n",
        "## store the tokenized reviews in reviews_ints\n",
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "  reviews_ints.append([vocab_to_int[word] for word in review.split()])"
      ],
      "metadata": {
        "id": "-XyCMC2O4S_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test if it works fine"
      ],
      "metadata": {
        "id": "b0Xb62u24d2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stats about vocabulary\n",
        "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
        "print()\n",
        "\n",
        "# print tokens in first review\n",
        "print('Tokenized review: \\n', reviews_ints[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8kgQsvU4haz",
        "outputId": "039945eb-f44c-4c89-ab77-3ddfa6797d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words:  74072\n",
            "\n",
            "Tokenized review: \n",
            " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding the labels to 0 and 1"
      ],
      "metadata": {
        "id": "Ebnu8xAR6Yh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1=positive, 0=negative label conversion\n",
        "labels_split = labels.split('\\n')\n",
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
      ],
      "metadata": {
        "id": "OgQ0jGdr4ppg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if GPU is available for training"
      ],
      "metadata": {
        "id": "O5B0Se25TZvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IysTtfTrNDJe",
        "outputId": "1a9477ab-8e34-4e75-e711-8be739ec426d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, training on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: Remove the seqeunces with 0 length."
      ],
      "metadata": {
        "id": "sWwPMKqAThiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(reviews_ints)):\n",
        "  if len(reviews_ints[i]) == 0:\n",
        "    reviews_ints.remove(reviews_ints[i])\n",
        "    print(i)\n",
        "\n",
        "encoded_labels = np.array(encoded_labels)\n",
        "encoded_labels = encoded_labels[:25000]"
      ],
      "metadata": {
        "id": "I_fRGih7UKLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80349860-a469-40ee-d124-086ce3f30103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: Create padded sequences with the equal length, e.g., 100. You can choose the sequence length here. If the sequence length is larger than your choosen length, you will truncate it so that all sequences have the same length (after 0 padding for shorter sequences)."
      ],
      "metadata": {
        "id": "nLPI4QohULG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "reviews_ints_copy = deepcopy(reviews_ints)\n",
        "padding = 100\n",
        "\n",
        "for i in range(len(reviews_ints_copy)):\n",
        "  if len(reviews_ints_copy[i]) < padding:\n",
        "    count = len(reviews_ints_copy[i])\n",
        "    for j in range(padding - count):\n",
        "      reviews_ints_copy[i] = np.append(reviews_ints_copy[i], 0)\n",
        "  elif len(reviews_ints_copy[i]) > padding:\n",
        "    reviews_ints_copy[i] = reviews_ints_copy[i][:padding]\n",
        "\n",
        "reviews_ints_copy = np.vstack(reviews_ints_copy)\n",
        "\n"
      ],
      "metadata": {
        "id": "JEH5EnFzUOuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: Create your own dataset class definition (inherited from torch.utils.data.Dataset) and use DataLoader"
      ],
      "metadata": {
        "id": "E2Z0OrM9UgU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Sentiment(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.X[index]\n",
        "        Y = self.Y[index]\n",
        "        return X, Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "\n",
        "training_data = Sentiment(reviews_ints_copy[:20000], encoded_labels[:20000])\n",
        "test_data = Sentiment(reviews_ints_copy[20000:], encoded_labels[20000:])\n",
        "\n",
        "train_loader = DataLoader(training_data, batch_size=500, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=500, shuffle=True)"
      ],
      "metadata": {
        "id": "euDlSwVGVOaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: Create your own RNN model class definition (inherited from torch.nn.Module)"
      ],
      "metadata": {
        "id": "AduMqtABVsox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "\n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "metadata": {
        "id": "N75GM5nQVuKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: Create your code for training the model"
      ],
      "metadata": {
        "id": "e5JZNtMyWGNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: Create your code for testing (evaluation on test set)\n"
      ],
      "metadata": {
        "id": "gUzQRsJKdVh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import binarize\n",
        "\n",
        "model = RNN(len(vocab_to_int)+1, 256, 1024, 1)\n",
        "model.eval()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for data, label in train_loader:\n",
        "\n",
        "    output = model(data.T)\n",
        "    loss = criterion(output.flatten(), label.flatten().type(torch.FloatTensor))\n",
        "    print(\"Training loss : {}\".format(loss))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  hit = 0\n",
        "  tot = 0\n",
        "  with torch.no_grad():\n",
        "    for data, label in test_loader:\n",
        "\n",
        "      outputs = model(data.T)\n",
        "      output = outputs.flatten().detach().cpu().numpy()\n",
        "      output_n = binarize(output.reshape(-1,1), threshold=0)\n",
        "      labels_n = label.flatten().detach().cpu().numpy()\n",
        "\n",
        "      for i in range(output_n.shape[0]):\n",
        "        if output_n[i] == labels_n[i]:\n",
        "          hit += 1\n",
        "          tot += 1\n",
        "        else:\n",
        "          tot += 1\n",
        "    print(\"===========================================================\")\n",
        "    print(\"Test Acc on Epoch {}\".format(epoch) + \": {}\".format(hit/tot))\n",
        "    print(\"===========================================================\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2wepp6QrWLN1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc8e082f-4350-4686-c0c1-ea3cf9980ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss : 0.6938515901565552\n",
            "Training loss : 0.7019339799880981\n",
            "Training loss : 0.7047690153121948\n",
            "Training loss : 0.6985001564025879\n",
            "Training loss : 0.6951850056648254\n",
            "Training loss : 0.6979073286056519\n",
            "Training loss : 0.6938180923461914\n",
            "Training loss : 0.693930983543396\n",
            "Training loss : 0.6901216506958008\n",
            "Training loss : 0.6895695924758911\n",
            "Training loss : 0.6981564164161682\n",
            "Training loss : 0.700493574142456\n",
            "Training loss : 0.6887861490249634\n",
            "Training loss : 0.7025400996208191\n",
            "Training loss : 0.6959770321846008\n",
            "Training loss : 0.6928430199623108\n",
            "Training loss : 0.6883878111839294\n",
            "Training loss : 0.6989741325378418\n",
            "Training loss : 0.6994673013687134\n",
            "Training loss : 0.6907265186309814\n",
            "Training loss : 0.6975096464157104\n",
            "Training loss : 0.7014065980911255\n",
            "Training loss : 0.6908881664276123\n",
            "Training loss : 0.6961145997047424\n",
            "Training loss : 0.6893723607063293\n",
            "Training loss : 0.6976418495178223\n",
            "Training loss : 0.693834125995636\n",
            "Training loss : 0.6960244178771973\n",
            "Training loss : 0.6975860595703125\n",
            "Training loss : 0.6904841661453247\n",
            "Training loss : 0.6962071657180786\n",
            "Training loss : 0.6935060620307922\n",
            "Training loss : 0.6860225200653076\n",
            "Training loss : 0.7035982608795166\n",
            "Training loss : 0.7000381946563721\n",
            "Training loss : 0.6897905468940735\n",
            "Training loss : 0.6957352161407471\n",
            "Training loss : 0.6988808512687683\n",
            "Training loss : 0.6866428256034851\n",
            "Training loss : 0.6888511776924133\n",
            "===========================================================\n",
            "Test Acc on Epoch 0: 0.5242\n",
            "===========================================================\n",
            "Training loss : 0.6845656633377075\n",
            "Training loss : 0.6789499521255493\n",
            "Training loss : 0.6782347559928894\n",
            "Training loss : 0.6922621726989746\n",
            "Training loss : 0.6774599552154541\n",
            "Training loss : 0.680252194404602\n",
            "Training loss : 0.6755061745643616\n",
            "Training loss : 0.6762082576751709\n",
            "Training loss : 0.6741309762001038\n",
            "Training loss : 0.6840327978134155\n",
            "Training loss : 0.6716564893722534\n",
            "Training loss : 0.6777930855751038\n",
            "Training loss : 0.6914289593696594\n",
            "Training loss : 0.6705097556114197\n",
            "Training loss : 0.6773001551628113\n",
            "Training loss : 0.6788343787193298\n",
            "Training loss : 0.6820043921470642\n",
            "Training loss : 0.6835488080978394\n",
            "Training loss : 0.6710629463195801\n",
            "Training loss : 0.680772602558136\n",
            "Training loss : 0.6726972460746765\n",
            "Training loss : 0.6785618662834167\n",
            "Training loss : 0.6668657660484314\n",
            "Training loss : 0.6678521633148193\n",
            "Training loss : 0.6665449142456055\n",
            "Training loss : 0.673259973526001\n",
            "Training loss : 0.6836370825767517\n",
            "Training loss : 0.6736055612564087\n",
            "Training loss : 0.6749019026756287\n",
            "Training loss : 0.6664642691612244\n",
            "Training loss : 0.6699748039245605\n",
            "Training loss : 0.6692600250244141\n",
            "Training loss : 0.6554763317108154\n",
            "Training loss : 0.637325644493103\n",
            "Training loss : 0.6608624458312988\n",
            "Training loss : 0.6585188508033752\n",
            "Training loss : 0.6490346789360046\n",
            "Training loss : 0.6689333915710449\n",
            "Training loss : 0.6530038118362427\n",
            "Training loss : 0.6409907341003418\n",
            "===========================================================\n",
            "Test Acc on Epoch 1: 0.6276\n",
            "===========================================================\n",
            "Training loss : 0.625175952911377\n",
            "Training loss : 0.631576657295227\n",
            "Training loss : 0.633668065071106\n",
            "Training loss : 0.6338230967521667\n",
            "Training loss : 0.6176372170448303\n",
            "Training loss : 0.604472279548645\n",
            "Training loss : 0.5895349383354187\n",
            "Training loss : 0.6031295657157898\n",
            "Training loss : 0.6164684891700745\n",
            "Training loss : 0.6086864471435547\n",
            "Training loss : 0.5980814099311829\n",
            "Training loss : 0.6587250828742981\n",
            "Training loss : 0.6351233124732971\n",
            "Training loss : 0.6227977275848389\n",
            "Training loss : 0.6367834806442261\n",
            "Training loss : 0.6137357354164124\n",
            "Training loss : 0.6163365840911865\n",
            "Training loss : 0.6118629574775696\n",
            "Training loss : 0.6206525564193726\n",
            "Training loss : 0.6275954842567444\n",
            "Training loss : 0.6330007314682007\n",
            "Training loss : 0.6206610798835754\n",
            "Training loss : 0.6188825964927673\n",
            "Training loss : 0.6263702511787415\n",
            "Training loss : 0.5923119783401489\n",
            "Training loss : 0.6224055886268616\n",
            "Training loss : 0.6168747544288635\n",
            "Training loss : 0.6146643161773682\n",
            "Training loss : 0.6388108134269714\n",
            "Training loss : 0.6395527720451355\n",
            "Training loss : 0.6275746822357178\n",
            "Training loss : 0.6115396022796631\n",
            "Training loss : 0.6145581603050232\n",
            "Training loss : 0.5996274352073669\n",
            "Training loss : 0.5975845456123352\n",
            "Training loss : 0.6169911623001099\n",
            "Training loss : 0.6243794560432434\n",
            "Training loss : 0.6070173382759094\n",
            "Training loss : 0.6380416750907898\n",
            "Training loss : 0.6240252256393433\n",
            "===========================================================\n",
            "Test Acc on Epoch 2: 0.6548\n",
            "===========================================================\n",
            "Training loss : 0.5700886845588684\n",
            "Training loss : 0.5731831192970276\n",
            "Training loss : 0.6242579221725464\n",
            "Training loss : 0.609278678894043\n",
            "Training loss : 0.6099708676338196\n",
            "Training loss : 0.6087906956672668\n",
            "Training loss : 0.6272293329238892\n",
            "Training loss : 0.619462788105011\n",
            "Training loss : 0.6085794568061829\n",
            "Training loss : 0.5788869857788086\n",
            "Training loss : 0.6039892435073853\n",
            "Training loss : 0.5796375870704651\n",
            "Training loss : 0.5941756367683411\n",
            "Training loss : 0.5897393822669983\n",
            "Training loss : 0.5541375279426575\n",
            "Training loss : 0.5946676731109619\n",
            "Training loss : 0.5770358443260193\n",
            "Training loss : 0.5855383276939392\n",
            "Training loss : 0.575677752494812\n",
            "Training loss : 0.6055295467376709\n",
            "Training loss : 0.5572184920310974\n",
            "Training loss : 0.6221503615379333\n",
            "Training loss : 0.5904907584190369\n",
            "Training loss : 0.5545862317085266\n",
            "Training loss : 0.5830414891242981\n",
            "Training loss : 0.6003071665763855\n",
            "Training loss : 0.5850483179092407\n",
            "Training loss : 0.6019743084907532\n",
            "Training loss : 0.5803654193878174\n",
            "Training loss : 0.5949468016624451\n",
            "Training loss : 0.5522867441177368\n",
            "Training loss : 0.5640482306480408\n",
            "Training loss : 0.6016610264778137\n",
            "Training loss : 0.6002978086471558\n",
            "Training loss : 0.6073167324066162\n",
            "Training loss : 0.6074478030204773\n",
            "Training loss : 0.6079060435295105\n",
            "Training loss : 0.5901561975479126\n",
            "Training loss : 0.5865054726600647\n",
            "Training loss : 0.6035963296890259\n",
            "===========================================================\n",
            "Test Acc on Epoch 3: 0.681\n",
            "===========================================================\n",
            "Training loss : 0.5688457489013672\n",
            "Training loss : 0.5688880681991577\n",
            "Training loss : 0.5405526757240295\n",
            "Training loss : 0.5804719924926758\n",
            "Training loss : 0.5680109858512878\n",
            "Training loss : 0.5537320375442505\n",
            "Training loss : 0.5431712865829468\n",
            "Training loss : 0.5680866241455078\n",
            "Training loss : 0.559715986251831\n",
            "Training loss : 0.5706316232681274\n",
            "Training loss : 0.5973777174949646\n",
            "Training loss : 0.5826441645622253\n",
            "Training loss : 0.6131775379180908\n",
            "Training loss : 0.6107093691825867\n",
            "Training loss : 0.6263823509216309\n",
            "Training loss : 0.5966634154319763\n",
            "Training loss : 0.5718487501144409\n",
            "Training loss : 0.5907643437385559\n",
            "Training loss : 0.602798342704773\n",
            "Training loss : 0.5688297152519226\n",
            "Training loss : 0.5665706992149353\n",
            "Training loss : 0.5814443826675415\n",
            "Training loss : 0.5928181409835815\n",
            "Training loss : 0.5819911956787109\n",
            "Training loss : 0.5628402829170227\n",
            "Training loss : 0.5950637459754944\n",
            "Training loss : 0.56660395860672\n",
            "Training loss : 0.5731359124183655\n",
            "Training loss : 0.5898478031158447\n",
            "Training loss : 0.611738920211792\n",
            "Training loss : 0.6011269688606262\n",
            "Training loss : 0.5998489260673523\n",
            "Training loss : 0.5725168585777283\n",
            "Training loss : 0.575890064239502\n",
            "Training loss : 0.5857343077659607\n",
            "Training loss : 0.6303901076316833\n",
            "Training loss : 0.6400100588798523\n",
            "Training loss : 0.5477471947669983\n",
            "Training loss : 0.5563766360282898\n",
            "Training loss : 0.6184830665588379\n",
            "===========================================================\n",
            "Test Acc on Epoch 4: 0.666\n",
            "===========================================================\n",
            "Training loss : 0.5736470222473145\n",
            "Training loss : 0.5638096928596497\n",
            "Training loss : 0.5838082432746887\n",
            "Training loss : 0.5351027846336365\n",
            "Training loss : 0.5657151341438293\n",
            "Training loss : 0.5622599124908447\n",
            "Training loss : 0.5646410584449768\n",
            "Training loss : 0.5663477778434753\n",
            "Training loss : 0.5527632236480713\n",
            "Training loss : 0.515222430229187\n",
            "Training loss : 0.5337097644805908\n",
            "Training loss : 0.5389783978462219\n",
            "Training loss : 0.5279586315155029\n",
            "Training loss : 0.5521062612533569\n",
            "Training loss : 0.5409951210021973\n",
            "Training loss : 0.5839709043502808\n",
            "Training loss : 0.520665168762207\n",
            "Training loss : 0.5730990171432495\n",
            "Training loss : 0.5155418515205383\n",
            "Training loss : 0.5462054014205933\n",
            "Training loss : 0.5490248203277588\n",
            "Training loss : 0.559046745300293\n",
            "Training loss : 0.5870707035064697\n",
            "Training loss : 0.5979634523391724\n",
            "Training loss : 0.5663353204727173\n",
            "Training loss : 0.5696409940719604\n",
            "Training loss : 0.6104426980018616\n",
            "Training loss : 0.5765402317047119\n",
            "Training loss : 0.5625942349433899\n",
            "Training loss : 0.5798918008804321\n",
            "Training loss : 0.5957574248313904\n",
            "Training loss : 0.5424667596817017\n",
            "Training loss : 0.5334765911102295\n",
            "Training loss : 0.5427917242050171\n",
            "Training loss : 0.5507948398590088\n",
            "Training loss : 0.5422111749649048\n",
            "Training loss : 0.6102858185768127\n",
            "Training loss : 0.6042685508728027\n",
            "Training loss : 0.5929383635520935\n",
            "Training loss : 0.5659504532814026\n",
            "===========================================================\n",
            "Test Acc on Epoch 5: 0.6942\n",
            "===========================================================\n",
            "Training loss : 0.5166863799095154\n",
            "Training loss : 0.5384309887886047\n",
            "Training loss : 0.494241327047348\n",
            "Training loss : 0.5129275321960449\n",
            "Training loss : 0.5539993047714233\n",
            "Training loss : 0.5235678553581238\n",
            "Training loss : 0.5331313014030457\n",
            "Training loss : 0.5450376272201538\n",
            "Training loss : 0.5327306985855103\n",
            "Training loss : 0.5439468622207642\n",
            "Training loss : 0.5244302749633789\n",
            "Training loss : 0.48155710101127625\n",
            "Training loss : 0.6408424973487854\n",
            "Training loss : 0.6099344491958618\n",
            "Training loss : 0.5781640410423279\n",
            "Training loss : 0.515407383441925\n",
            "Training loss : 0.6101015210151672\n",
            "Training loss : 0.5868632793426514\n",
            "Training loss : 0.5817410945892334\n",
            "Training loss : 0.5875054001808167\n",
            "Training loss : 0.5673269033432007\n",
            "Training loss : 0.5617570877075195\n",
            "Training loss : 0.54954594373703\n",
            "Training loss : 0.555848240852356\n",
            "Training loss : 0.5513036251068115\n",
            "Training loss : 0.5992390513420105\n",
            "Training loss : 0.5677238702774048\n",
            "Training loss : 0.5523122549057007\n",
            "Training loss : 0.5596491694450378\n",
            "Training loss : 0.5342644453048706\n",
            "Training loss : 0.5504558086395264\n",
            "Training loss : 0.555965006351471\n",
            "Training loss : 0.5377553701400757\n",
            "Training loss : 0.5729858875274658\n",
            "Training loss : 0.5700168013572693\n",
            "Training loss : 0.5263116359710693\n",
            "Training loss : 0.5909071564674377\n",
            "Training loss : 0.5879631638526917\n",
            "Training loss : 0.5198743939399719\n",
            "Training loss : 0.5125359892845154\n",
            "===========================================================\n",
            "Test Acc on Epoch 6: 0.6794\n",
            "===========================================================\n",
            "Training loss : 0.5110617280006409\n",
            "Training loss : 0.4949176013469696\n",
            "Training loss : 0.5369976162910461\n",
            "Training loss : 0.4782947301864624\n",
            "Training loss : 0.49794983863830566\n",
            "Training loss : 0.5432971715927124\n",
            "Training loss : 0.531120777130127\n",
            "Training loss : 0.5273657441139221\n",
            "Training loss : 0.4940044581890106\n",
            "Training loss : 0.49576255679130554\n",
            "Training loss : 0.5223464965820312\n",
            "Training loss : 0.5379084348678589\n",
            "Training loss : 0.49650949239730835\n",
            "Training loss : 0.5337433815002441\n",
            "Training loss : 0.5252987742424011\n",
            "Training loss : 0.555624783039093\n",
            "Training loss : 0.5154992341995239\n",
            "Training loss : 0.5701712369918823\n",
            "Training loss : 0.57051020860672\n",
            "Training loss : 0.5532853007316589\n",
            "Training loss : 0.556136965751648\n",
            "Training loss : 0.5580546259880066\n",
            "Training loss : 0.5044472217559814\n",
            "Training loss : 0.5626527070999146\n",
            "Training loss : 0.5296695828437805\n",
            "Training loss : 0.5378356575965881\n",
            "Training loss : 0.550459623336792\n",
            "Training loss : 0.5222896933555603\n",
            "Training loss : 0.5412182211875916\n",
            "Training loss : 0.5314809083938599\n",
            "Training loss : 0.5890420079231262\n",
            "Training loss : 0.5695384740829468\n",
            "Training loss : 0.5741329193115234\n",
            "Training loss : 0.54099041223526\n",
            "Training loss : 0.5840188264846802\n",
            "Training loss : 0.5738102197647095\n",
            "Training loss : 0.5587491989135742\n",
            "Training loss : 0.568289041519165\n",
            "Training loss : 0.573908269405365\n",
            "Training loss : 0.5835239291191101\n",
            "===========================================================\n",
            "Test Acc on Epoch 7: 0.6398\n",
            "===========================================================\n",
            "Training loss : 0.5385309457778931\n",
            "Training loss : 0.531787097454071\n",
            "Training loss : 0.5768455266952515\n",
            "Training loss : 0.5568819046020508\n",
            "Training loss : 0.5512809157371521\n",
            "Training loss : 0.547286331653595\n",
            "Training loss : 0.5328325629234314\n",
            "Training loss : 0.5418042540550232\n",
            "Training loss : 0.5154346823692322\n",
            "Training loss : 0.5213326215744019\n",
            "Training loss : 0.5491442084312439\n",
            "Training loss : 0.5220685005187988\n",
            "Training loss : 0.5216164588928223\n",
            "Training loss : 0.5166482925415039\n",
            "Training loss : 0.5117796659469604\n",
            "Training loss : 0.5519553422927856\n",
            "Training loss : 0.5469691157341003\n",
            "Training loss : 0.5366977453231812\n",
            "Training loss : 0.5488898158073425\n",
            "Training loss : 0.5548694133758545\n",
            "Training loss : 0.5173085331916809\n",
            "Training loss : 0.5295972228050232\n",
            "Training loss : 0.5276523232460022\n",
            "Training loss : 0.5358424186706543\n",
            "Training loss : 0.5716989636421204\n",
            "Training loss : 0.5329084396362305\n",
            "Training loss : 0.5311722159385681\n",
            "Training loss : 0.5589953064918518\n",
            "Training loss : 0.5204529762268066\n",
            "Training loss : 0.537415623664856\n",
            "Training loss : 0.5289605855941772\n",
            "Training loss : 0.5209718942642212\n",
            "Training loss : 0.5220255255699158\n",
            "Training loss : 0.5401195287704468\n",
            "Training loss : 0.5510095953941345\n",
            "Training loss : 0.5289924740791321\n",
            "Training loss : 0.524031400680542\n",
            "Training loss : 0.4899108409881592\n",
            "Training loss : 0.5082054734230042\n",
            "Training loss : 0.49275168776512146\n",
            "===========================================================\n",
            "Test Acc on Epoch 8: 0.6736\n",
            "===========================================================\n",
            "Training loss : 0.47644391655921936\n",
            "Training loss : 0.5154160261154175\n",
            "Training loss : 0.5323294401168823\n",
            "Training loss : 0.4773283302783966\n",
            "Training loss : 0.513637125492096\n",
            "Training loss : 0.496127188205719\n",
            "Training loss : 0.49896663427352905\n",
            "Training loss : 0.48117852210998535\n",
            "Training loss : 0.5087195634841919\n",
            "Training loss : 0.5074623227119446\n",
            "Training loss : 0.45841696858406067\n",
            "Training loss : 0.5186887383460999\n",
            "Training loss : 0.5148921012878418\n",
            "Training loss : 0.5606822371482849\n",
            "Training loss : 0.5025826692581177\n",
            "Training loss : 0.5150489211082458\n",
            "Training loss : 0.47820720076560974\n",
            "Training loss : 0.5249953866004944\n",
            "Training loss : 0.4909006953239441\n",
            "Training loss : 0.508167564868927\n",
            "Training loss : 0.4836384356021881\n",
            "Training loss : 0.42852798104286194\n",
            "Training loss : 0.47244635224342346\n",
            "Training loss : 0.4690863788127899\n",
            "Training loss : 0.5168581604957581\n",
            "Training loss : 0.4747205078601837\n",
            "Training loss : 0.5524135828018188\n",
            "Training loss : 0.524824857711792\n",
            "Training loss : 0.4840211272239685\n",
            "Training loss : 0.5258230566978455\n",
            "Training loss : 0.5068567991256714\n",
            "Training loss : 0.5168636441230774\n",
            "Training loss : 0.4828549027442932\n",
            "Training loss : 0.5649843811988831\n",
            "Training loss : 0.49965736269950867\n",
            "Training loss : 0.4809139668941498\n",
            "Training loss : 0.49054190516471863\n",
            "Training loss : 0.5159944891929626\n",
            "Training loss : 0.4966830313205719\n",
            "Training loss : 0.4690732955932617\n",
            "===========================================================\n",
            "Test Acc on Epoch 9: 0.6924\n",
            "===========================================================\n",
            "Training loss : 0.5002294778823853\n",
            "Training loss : 0.5131291151046753\n",
            "Training loss : 0.5120221376419067\n",
            "Training loss : 0.4971599876880646\n",
            "Training loss : 0.51820307970047\n",
            "Training loss : 0.46968916058540344\n",
            "Training loss : 0.48576995730400085\n",
            "Training loss : 0.49565210938453674\n",
            "Training loss : 0.5131666660308838\n",
            "Training loss : 0.46526095271110535\n",
            "Training loss : 0.47031164169311523\n",
            "Training loss : 0.458412230014801\n",
            "Training loss : 0.4690142869949341\n",
            "Training loss : 0.4653538167476654\n",
            "Training loss : 0.4893129765987396\n",
            "Training loss : 0.47818660736083984\n",
            "Training loss : 0.5054870843887329\n",
            "Training loss : 0.5095482468605042\n",
            "Training loss : 0.50285404920578\n",
            "Training loss : 0.4732360541820526\n",
            "Training loss : 0.5127803683280945\n",
            "Training loss : 0.4781125485897064\n",
            "Training loss : 0.5134705305099487\n",
            "Training loss : 0.44946688413619995\n",
            "Training loss : 0.4436184763908386\n",
            "Training loss : 0.4857443869113922\n",
            "Training loss : 0.5592213869094849\n",
            "Training loss : 0.5420105457305908\n",
            "Training loss : 0.5534512400627136\n",
            "Training loss : 0.5784323215484619\n",
            "Training loss : 0.5605777502059937\n",
            "Training loss : 0.5636292695999146\n",
            "Training loss : 0.5489329695701599\n",
            "Training loss : 0.4711246192455292\n",
            "Training loss : 0.4775118827819824\n",
            "Training loss : 0.554512083530426\n",
            "Training loss : 0.5285335779190063\n",
            "Training loss : 0.4804357886314392\n",
            "Training loss : 0.5261842012405396\n",
            "Training loss : 0.4794967770576477\n",
            "===========================================================\n",
            "Test Acc on Epoch 10: 0.6822\n",
            "===========================================================\n",
            "Training loss : 0.4904482066631317\n",
            "Training loss : 0.5022462606430054\n",
            "Training loss : 0.48574304580688477\n",
            "Training loss : 0.5035194158554077\n",
            "Training loss : 0.5174550414085388\n",
            "Training loss : 0.5095947980880737\n",
            "Training loss : 0.5201526880264282\n",
            "Training loss : 0.5055772066116333\n",
            "Training loss : 0.45479023456573486\n",
            "Training loss : 0.4862363338470459\n",
            "Training loss : 0.4366467297077179\n",
            "Training loss : 0.4471336603164673\n",
            "Training loss : 0.4979947507381439\n",
            "Training loss : 0.49548012018203735\n",
            "Training loss : 0.5010543465614319\n",
            "Training loss : 0.491317480802536\n",
            "Training loss : 0.4890037775039673\n",
            "Training loss : 0.47347700595855713\n",
            "Training loss : 0.49157485365867615\n",
            "Training loss : 0.47664111852645874\n",
            "Training loss : 0.5196795463562012\n",
            "Training loss : 0.48954829573631287\n",
            "Training loss : 0.4567895531654358\n",
            "Training loss : 0.4804229736328125\n",
            "Training loss : 0.5058517456054688\n",
            "Training loss : 0.5001403093338013\n",
            "Training loss : 0.460052490234375\n",
            "Training loss : 0.4806779623031616\n",
            "Training loss : 0.5355914831161499\n",
            "Training loss : 0.5220860242843628\n",
            "Training loss : 0.49494022130966187\n",
            "Training loss : 0.48875659704208374\n",
            "Training loss : 0.49941977858543396\n",
            "Training loss : 0.4734472334384918\n",
            "Training loss : 0.5112091302871704\n",
            "Training loss : 0.4921804368495941\n",
            "Training loss : 0.5485730767250061\n",
            "Training loss : 0.47368088364601135\n",
            "Training loss : 0.5062645077705383\n",
            "Training loss : 0.4587494432926178\n",
            "===========================================================\n",
            "Test Acc on Epoch 11: 0.687\n",
            "===========================================================\n",
            "Training loss : 0.45020291209220886\n",
            "Training loss : 0.4701947569847107\n",
            "Training loss : 0.46365538239479065\n",
            "Training loss : 0.4451127350330353\n",
            "Training loss : 0.4606843888759613\n",
            "Training loss : 0.4473244249820709\n",
            "Training loss : 0.440349817276001\n",
            "Training loss : 0.4577661156654358\n",
            "Training loss : 0.49600744247436523\n",
            "Training loss : 0.5126929879188538\n",
            "Training loss : 0.4825676679611206\n",
            "Training loss : 0.430053174495697\n",
            "Training loss : 0.49711915850639343\n",
            "Training loss : 0.46579205989837646\n",
            "Training loss : 0.4444792568683624\n",
            "Training loss : 0.4638371765613556\n",
            "Training loss : 0.4650719463825226\n",
            "Training loss : 0.4678255319595337\n",
            "Training loss : 0.4613451659679413\n",
            "Training loss : 0.43840327858924866\n",
            "Training loss : 0.4721316993236542\n",
            "Training loss : 0.5051288604736328\n",
            "Training loss : 0.4846348464488983\n",
            "Training loss : 0.4980838894844055\n",
            "Training loss : 0.45800313353538513\n",
            "Training loss : 0.47734710574150085\n",
            "Training loss : 0.49189627170562744\n",
            "Training loss : 0.5067902207374573\n",
            "Training loss : 0.49868911504745483\n",
            "Training loss : 0.439710795879364\n",
            "Training loss : 0.4931105375289917\n",
            "Training loss : 0.48530423641204834\n",
            "Training loss : 0.4406370222568512\n",
            "Training loss : 0.5232535600662231\n",
            "Training loss : 0.4803576171398163\n",
            "Training loss : 0.47085461020469666\n",
            "Training loss : 0.5155711770057678\n",
            "Training loss : 0.4678249955177307\n",
            "Training loss : 0.4451657831668854\n",
            "Training loss : 0.43076857924461365\n",
            "===========================================================\n",
            "Test Acc on Epoch 12: 0.697\n",
            "===========================================================\n",
            "Training loss : 0.44987359642982483\n",
            "Training loss : 0.4385427236557007\n",
            "Training loss : 0.4699772000312805\n",
            "Training loss : 0.519705593585968\n",
            "Training loss : 0.5415958166122437\n",
            "Training loss : 0.5587623715400696\n",
            "Training loss : 0.5338382124900818\n",
            "Training loss : 0.49437275528907776\n",
            "Training loss : 0.44755956530570984\n",
            "Training loss : 0.45514532923698425\n",
            "Training loss : 0.48567381501197815\n",
            "Training loss : 0.5285128355026245\n",
            "Training loss : 0.4842138886451721\n",
            "Training loss : 0.5220577120780945\n",
            "Training loss : 0.4962509274482727\n",
            "Training loss : 0.4868820607662201\n",
            "Training loss : 0.4846985638141632\n",
            "Training loss : 0.4814740717411041\n",
            "Training loss : 0.48310887813568115\n",
            "Training loss : 0.5224039554595947\n",
            "Training loss : 0.47783491015434265\n",
            "Training loss : 0.44053149223327637\n",
            "Training loss : 0.453538715839386\n",
            "Training loss : 0.505739152431488\n",
            "Training loss : 0.46107909083366394\n",
            "Training loss : 0.45280611515045166\n",
            "Training loss : 0.4857279658317566\n",
            "Training loss : 0.4576064348220825\n",
            "Training loss : 0.48112592101097107\n",
            "Training loss : 0.4521493911743164\n",
            "Training loss : 0.4634121358394623\n",
            "Training loss : 0.4400175213813782\n",
            "Training loss : 0.4726486802101135\n",
            "Training loss : 0.460416316986084\n",
            "Training loss : 0.43676063418388367\n",
            "Training loss : 0.44151660799980164\n",
            "Training loss : 0.4593082666397095\n",
            "Training loss : 0.46377840638160706\n",
            "Training loss : 0.4630659222602844\n",
            "Training loss : 0.4740995466709137\n",
            "===========================================================\n",
            "Test Acc on Epoch 13: 0.6954\n",
            "===========================================================\n",
            "Training loss : 0.40628790855407715\n",
            "Training loss : 0.3989947736263275\n",
            "Training loss : 0.4288078844547272\n",
            "Training loss : 0.4166359603404999\n",
            "Training loss : 0.4181225299835205\n",
            "Training loss : 0.4207584261894226\n",
            "Training loss : 0.4500284790992737\n",
            "Training loss : 0.5228533744812012\n",
            "Training loss : 0.43326419591903687\n",
            "Training loss : 0.42805376648902893\n",
            "Training loss : 0.3951603174209595\n",
            "Training loss : 0.4271618127822876\n",
            "Training loss : 0.45664551854133606\n",
            "Training loss : 0.4552546739578247\n",
            "Training loss : 0.4545707106590271\n",
            "Training loss : 0.4491025507450104\n",
            "Training loss : 0.5012362599372864\n",
            "Training loss : 0.46267634630203247\n",
            "Training loss : 0.5271883010864258\n",
            "Training loss : 0.45587480068206787\n",
            "Training loss : 0.46518683433532715\n",
            "Training loss : 0.49494630098342896\n",
            "Training loss : 0.3918289244174957\n",
            "Training loss : 0.4909204840660095\n",
            "Training loss : 0.44981276988983154\n",
            "Training loss : 0.4500064253807068\n",
            "Training loss : 0.4569830000400543\n",
            "Training loss : 0.4654066860675812\n",
            "Training loss : 0.4568271338939667\n",
            "Training loss : 0.4575711786746979\n",
            "Training loss : 0.41572803258895874\n",
            "Training loss : 0.4477884769439697\n",
            "Training loss : 0.445933997631073\n",
            "Training loss : 0.4334058165550232\n",
            "Training loss : 0.4420785903930664\n",
            "Training loss : 0.4188723862171173\n",
            "Training loss : 0.4931539297103882\n",
            "Training loss : 0.4812409281730652\n",
            "Training loss : 0.47114965319633484\n",
            "Training loss : 0.4267849028110504\n",
            "===========================================================\n",
            "Test Acc on Epoch 14: 0.6998\n",
            "===========================================================\n",
            "Training loss : 0.40242835879325867\n",
            "Training loss : 0.40108922123908997\n",
            "Training loss : 0.42247140407562256\n",
            "Training loss : 0.6387086510658264\n",
            "Training loss : 0.6379815936088562\n",
            "Training loss : 0.6382024884223938\n",
            "Training loss : 0.6198863983154297\n",
            "Training loss : 0.671069324016571\n",
            "Training loss : 0.6322146058082581\n",
            "Training loss : 0.6196416616439819\n",
            "Training loss : 0.6009718775749207\n",
            "Training loss : 0.6087787747383118\n",
            "Training loss : 0.6202261447906494\n",
            "Training loss : 0.6030789613723755\n",
            "Training loss : 0.6197946071624756\n",
            "Training loss : 0.6262902617454529\n",
            "Training loss : 0.6625499129295349\n",
            "Training loss : 0.6690749526023865\n",
            "Training loss : 0.6077630519866943\n",
            "Training loss : 0.6260085105895996\n",
            "Training loss : 0.6245934367179871\n",
            "Training loss : 0.6063184142112732\n",
            "Training loss : 0.6187736392021179\n",
            "Training loss : 0.6257035732269287\n",
            "Training loss : 0.6247149109840393\n",
            "Training loss : 0.6447346210479736\n",
            "Training loss : 0.6329008936882019\n",
            "Training loss : 0.6203611493110657\n",
            "Training loss : 0.5885021686553955\n",
            "Training loss : 0.6080899834632874\n",
            "Training loss : 0.6193519234657288\n",
            "Training loss : 0.5968895554542542\n",
            "Training loss : 0.6327656507492065\n",
            "Training loss : 0.5928089618682861\n",
            "Training loss : 0.594296395778656\n",
            "Training loss : 0.621143102645874\n",
            "Training loss : 0.6388264298439026\n",
            "Training loss : 0.6023606657981873\n",
            "Training loss : 0.6085515022277832\n",
            "Training loss : 0.6403080224990845\n",
            "===========================================================\n",
            "Test Acc on Epoch 15: 0.5312\n",
            "===========================================================\n",
            "Training loss : 0.6189303398132324\n",
            "Training loss : 0.6003963947296143\n",
            "Training loss : 0.5847353935241699\n",
            "Training loss : 0.5903438925743103\n",
            "Training loss : 0.5790320038795471\n",
            "Training loss : 0.5890924334526062\n",
            "Training loss : 0.5862653255462646\n",
            "Training loss : 0.5937116146087646\n",
            "Training loss : 0.5870566368103027\n",
            "Training loss : 0.5773654580116272\n",
            "Training loss : 0.5835042595863342\n",
            "Training loss : 0.583681583404541\n",
            "Training loss : 0.5848491191864014\n",
            "Training loss : 0.575627326965332\n",
            "Training loss : 0.5893492698669434\n",
            "Training loss : 0.5667016506195068\n",
            "Training loss : 0.5802025198936462\n",
            "Training loss : 0.5822396874427795\n",
            "Training loss : 0.5573218464851379\n",
            "Training loss : 0.5776854157447815\n",
            "Training loss : 0.579292893409729\n",
            "Training loss : 0.5859398245811462\n",
            "Training loss : 0.6126749515533447\n",
            "Training loss : 0.5956245064735413\n",
            "Training loss : 0.5452262163162231\n",
            "Training loss : 0.5327022671699524\n",
            "Training loss : 0.5476801991462708\n",
            "Training loss : 0.5665149092674255\n",
            "Training loss : 0.5149456858634949\n",
            "Training loss : 0.4771726727485657\n",
            "Training loss : 0.5006376504898071\n",
            "Training loss : 0.5049270391464233\n",
            "Training loss : 0.5176844596862793\n",
            "Training loss : 0.5291283130645752\n",
            "Training loss : 0.4842439889907837\n",
            "Training loss : 0.5275782346725464\n",
            "Training loss : 0.49410101771354675\n",
            "Training loss : 0.5097388625144958\n",
            "Training loss : 0.4903498589992523\n",
            "Training loss : 0.5408006906509399\n",
            "===========================================================\n",
            "Test Acc on Epoch 16: 0.6558\n",
            "===========================================================\n",
            "Training loss : 0.4365612268447876\n",
            "Training loss : 0.5066330432891846\n",
            "Training loss : 0.45116478204727173\n",
            "Training loss : 0.4257653057575226\n",
            "Training loss : 0.46282458305358887\n",
            "Training loss : 0.4872395396232605\n",
            "Training loss : 0.43794408440589905\n",
            "Training loss : 0.4773823022842407\n",
            "Training loss : 0.4456006586551666\n",
            "Training loss : 0.441745787858963\n",
            "Training loss : 0.4653051793575287\n",
            "Training loss : 0.453334778547287\n",
            "Training loss : 0.4467122554779053\n",
            "Training loss : 0.4497525990009308\n",
            "Training loss : 0.4907783269882202\n",
            "Training loss : 0.4185039699077606\n",
            "Training loss : 0.45373234152793884\n",
            "Training loss : 0.46637484431266785\n",
            "Training loss : 0.4441370964050293\n",
            "Training loss : 0.48513108491897583\n",
            "Training loss : 0.4449802041053772\n",
            "Training loss : 0.44864287972450256\n",
            "Training loss : 0.4333084523677826\n",
            "Training loss : 0.4580436944961548\n",
            "Training loss : 0.49579209089279175\n",
            "Training loss : 0.4602825939655304\n",
            "Training loss : 0.5001028776168823\n",
            "Training loss : 0.4521225094795227\n",
            "Training loss : 0.4588446319103241\n",
            "Training loss : 0.4808749854564667\n",
            "Training loss : 0.4352198541164398\n",
            "Training loss : 0.44654715061187744\n",
            "Training loss : 0.4333716630935669\n",
            "Training loss : 0.4237898290157318\n",
            "Training loss : 0.4158583879470825\n",
            "Training loss : 0.46356073021888733\n",
            "Training loss : 0.46430227160453796\n",
            "Training loss : 0.4359947144985199\n",
            "Training loss : 0.42648985981941223\n",
            "Training loss : 0.45906883478164673\n",
            "===========================================================\n",
            "Test Acc on Epoch 17: 0.679\n",
            "===========================================================\n",
            "Training loss : 0.4139333963394165\n",
            "Training loss : 0.4232109785079956\n",
            "Training loss : 0.39351412653923035\n",
            "Training loss : 0.4053952991962433\n",
            "Training loss : 0.46160760521888733\n",
            "Training loss : 0.4064573347568512\n",
            "Training loss : 0.45462456345558167\n",
            "Training loss : 0.3954482674598694\n",
            "Training loss : 0.39530229568481445\n",
            "Training loss : 0.4024689793586731\n",
            "Training loss : 0.40377601981163025\n",
            "Training loss : 0.38563302159309387\n",
            "Training loss : 0.4501364529132843\n",
            "Training loss : 0.4391505718231201\n",
            "Training loss : 0.44949889183044434\n",
            "Training loss : 0.41515472531318665\n",
            "Training loss : 0.43347862362861633\n",
            "Training loss : 0.3970107138156891\n",
            "Training loss : 0.4363348186016083\n",
            "Training loss : 0.4662259519100189\n",
            "Training loss : 0.3931504487991333\n",
            "Training loss : 0.43959376215934753\n",
            "Training loss : 0.44449883699417114\n",
            "Training loss : 0.43484392762184143\n",
            "Training loss : 0.4334104061126709\n",
            "Training loss : 0.47609400749206543\n",
            "Training loss : 0.42887669801712036\n",
            "Training loss : 0.42787808179855347\n",
            "Training loss : 0.38973018527030945\n",
            "Training loss : 0.4037992060184479\n",
            "Training loss : 0.42618075013160706\n",
            "Training loss : 0.44508978724479675\n",
            "Training loss : 0.41273176670074463\n",
            "Training loss : 0.4434775710105896\n",
            "Training loss : 0.40645843744277954\n",
            "Training loss : 0.38515427708625793\n",
            "Training loss : 0.3888819217681885\n",
            "Training loss : 0.39797160029411316\n",
            "Training loss : 0.4401721656322479\n",
            "Training loss : 0.419106125831604\n",
            "===========================================================\n",
            "Test Acc on Epoch 18: 0.7054\n",
            "===========================================================\n",
            "Training loss : 0.3754110634326935\n",
            "Training loss : 0.3391319513320923\n",
            "Training loss : 0.40113750100135803\n",
            "Training loss : 0.35192349553108215\n",
            "Training loss : 0.3797012269496918\n",
            "Training loss : 0.39041298627853394\n",
            "Training loss : 0.3696901500225067\n",
            "Training loss : 0.3965471088886261\n",
            "Training loss : 0.4032003879547119\n",
            "Training loss : 0.38019201159477234\n",
            "Training loss : 0.3629623055458069\n",
            "Training loss : 0.38956472277641296\n",
            "Training loss : 0.39606282114982605\n",
            "Training loss : 0.3929067850112915\n",
            "Training loss : 0.46591126918792725\n",
            "Training loss : 0.38059383630752563\n",
            "Training loss : 0.39654043316841125\n",
            "Training loss : 0.37957918643951416\n",
            "Training loss : 0.34585464000701904\n",
            "Training loss : 0.4422982335090637\n",
            "Training loss : 0.4308750033378601\n",
            "Training loss : 0.41371583938598633\n",
            "Training loss : 0.4599517285823822\n",
            "Training loss : 0.4754626452922821\n",
            "Training loss : 0.39357832074165344\n",
            "Training loss : 0.380243718624115\n",
            "Training loss : 0.39497315883636475\n",
            "Training loss : 0.34176093339920044\n",
            "Training loss : 0.3982703387737274\n",
            "Training loss : 0.4202481806278229\n",
            "Training loss : 0.4117940366268158\n",
            "Training loss : 0.39158254861831665\n",
            "Training loss : 0.4042590856552124\n",
            "Training loss : 0.4820050299167633\n",
            "Training loss : 0.41952985525131226\n",
            "Training loss : 0.43466755747795105\n",
            "Training loss : 0.4320377707481384\n",
            "Training loss : 0.4576493799686432\n",
            "Training loss : 0.3990563154220581\n",
            "Training loss : 0.41012170910835266\n",
            "===========================================================\n",
            "Test Acc on Epoch 19: 0.6808\n",
            "===========================================================\n",
            "Training loss : 0.38150039315223694\n",
            "Training loss : 0.40129315853118896\n",
            "Training loss : 0.39007872343063354\n",
            "Training loss : 0.3625530004501343\n",
            "Training loss : 0.34541842341423035\n",
            "Training loss : 0.3521747291088104\n",
            "Training loss : 0.4045182168483734\n",
            "Training loss : 0.38096940517425537\n",
            "Training loss : 0.3941693603992462\n",
            "Training loss : 0.3643413186073303\n",
            "Training loss : 0.35518553853034973\n",
            "Training loss : 0.39381760358810425\n",
            "Training loss : 0.3536630868911743\n",
            "Training loss : 0.3648017644882202\n",
            "Training loss : 0.40215298533439636\n",
            "Training loss : 0.3669191002845764\n",
            "Training loss : 0.40384623408317566\n",
            "Training loss : 0.35853880643844604\n",
            "Training loss : 0.33716535568237305\n",
            "Training loss : 0.43069419264793396\n",
            "Training loss : 0.41823944449424744\n",
            "Training loss : 0.33303794264793396\n",
            "Training loss : 0.35592877864837646\n",
            "Training loss : 0.3913167715072632\n",
            "Training loss : 0.37472495436668396\n",
            "Training loss : 0.3409084677696228\n",
            "Training loss : 0.3183424174785614\n",
            "Training loss : 0.3783935606479645\n",
            "Training loss : 0.41962748765945435\n",
            "Training loss : 0.3130021393299103\n",
            "Training loss : 0.4017099440097809\n",
            "Training loss : 0.465766966342926\n",
            "Training loss : 0.4100886881351471\n",
            "Training loss : 0.38286349177360535\n",
            "Training loss : 0.3991236984729767\n",
            "Training loss : 0.36007848381996155\n",
            "Training loss : 0.40387603640556335\n",
            "Training loss : 0.3909846842288971\n",
            "Training loss : 0.3854992091655731\n",
            "Training loss : 0.3671458661556244\n",
            "===========================================================\n",
            "Test Acc on Epoch 20: 0.6956\n",
            "===========================================================\n",
            "Training loss : 0.3565099537372589\n",
            "Training loss : 0.39223402738571167\n",
            "Training loss : 0.3867735266685486\n",
            "Training loss : 0.35999029874801636\n",
            "Training loss : 0.3526579737663269\n",
            "Training loss : 0.40203040838241577\n",
            "Training loss : 0.33782920241355896\n",
            "Training loss : 0.3777804970741272\n",
            "Training loss : 0.3840217888355255\n",
            "Training loss : 0.36759424209594727\n",
            "Training loss : 0.4470912516117096\n",
            "Training loss : 0.4438578188419342\n",
            "Training loss : 0.4376941919326782\n",
            "Training loss : 0.38161760568618774\n",
            "Training loss : 0.3850627839565277\n",
            "Training loss : 0.46334075927734375\n",
            "Training loss : 0.4338063597679138\n",
            "Training loss : 0.47344204783439636\n",
            "Training loss : 0.4721038341522217\n",
            "Training loss : 0.5077664256095886\n",
            "Training loss : 0.513681173324585\n",
            "Training loss : 0.49701911211013794\n",
            "Training loss : 0.41256988048553467\n",
            "Training loss : 0.47687411308288574\n",
            "Training loss : 0.4282532334327698\n",
            "Training loss : 0.4598408341407776\n",
            "Training loss : 0.39822259545326233\n",
            "Training loss : 0.43494299054145813\n",
            "Training loss : 0.4748498797416687\n",
            "Training loss : 0.4830766022205353\n",
            "Training loss : 0.4501257836818695\n",
            "Training loss : 0.38269612193107605\n",
            "Training loss : 0.37459269165992737\n",
            "Training loss : 0.33270251750946045\n",
            "Training loss : 0.41140276193618774\n",
            "Training loss : 0.3918994069099426\n",
            "Training loss : 0.40157970786094666\n",
            "Training loss : 0.40054139494895935\n",
            "Training loss : 0.3858354687690735\n",
            "Training loss : 0.38546937704086304\n",
            "===========================================================\n",
            "Test Acc on Epoch 21: 0.705\n",
            "===========================================================\n",
            "Training loss : 0.41988158226013184\n",
            "Training loss : 0.3867405354976654\n",
            "Training loss : 0.36299875378608704\n",
            "Training loss : 0.3630295395851135\n",
            "Training loss : 0.37788185477256775\n",
            "Training loss : 0.46022865176200867\n",
            "Training loss : 0.46765729784965515\n",
            "Training loss : 0.47453513741493225\n",
            "Training loss : 0.4654730260372162\n",
            "Training loss : 0.4499025046825409\n",
            "Training loss : 0.41357341408729553\n",
            "Training loss : 0.4460998475551605\n",
            "Training loss : 0.4767570495605469\n",
            "Training loss : 0.41545403003692627\n",
            "Training loss : 0.4660594165325165\n",
            "Training loss : 0.425151526927948\n",
            "Training loss : 0.45481523871421814\n",
            "Training loss : 0.4303625822067261\n",
            "Training loss : 0.44412270188331604\n",
            "Training loss : 0.47958868741989136\n",
            "Training loss : 0.4054681360721588\n",
            "Training loss : 0.44722843170166016\n",
            "Training loss : 0.40418270230293274\n",
            "Training loss : 0.41617894172668457\n",
            "Training loss : 0.4292524456977844\n",
            "Training loss : 0.4011839032173157\n",
            "Training loss : 0.4079057276248932\n",
            "Training loss : 0.4597163498401642\n",
            "Training loss : 0.4372459650039673\n",
            "Training loss : 0.41390636563301086\n",
            "Training loss : 0.4367363750934601\n",
            "Training loss : 0.44090649485588074\n",
            "Training loss : 0.3795919418334961\n",
            "Training loss : 0.42270779609680176\n",
            "Training loss : 0.4476810395717621\n",
            "Training loss : 0.3967166543006897\n",
            "Training loss : 0.42981767654418945\n",
            "Training loss : 0.3911026418209076\n",
            "Training loss : 0.40233245491981506\n",
            "Training loss : 0.4152226448059082\n",
            "===========================================================\n",
            "Test Acc on Epoch 22: 0.672\n",
            "===========================================================\n",
            "Training loss : 0.3538174331188202\n",
            "Training loss : 0.36042800545692444\n",
            "Training loss : 0.3730015754699707\n",
            "Training loss : 0.39696186780929565\n",
            "Training loss : 0.34794506430625916\n",
            "Training loss : 0.3838742971420288\n",
            "Training loss : 0.348239004611969\n",
            "Training loss : 0.34615692496299744\n",
            "Training loss : 0.35633930563926697\n",
            "Training loss : 0.3978712856769562\n",
            "Training loss : 0.36023667454719543\n",
            "Training loss : 0.3521231710910797\n",
            "Training loss : 0.34338560700416565\n",
            "Training loss : 0.37304267287254333\n",
            "Training loss : 0.36527660489082336\n",
            "Training loss : 0.32210075855255127\n",
            "Training loss : 0.3633458912372589\n",
            "Training loss : 0.3602721095085144\n",
            "Training loss : 0.3591063320636749\n",
            "Training loss : 0.3980770409107208\n",
            "Training loss : 0.36778783798217773\n",
            "Training loss : 0.38519057631492615\n",
            "Training loss : 0.36237633228302\n",
            "Training loss : 0.3988952040672302\n",
            "Training loss : 0.3513844907283783\n",
            "Training loss : 0.38651397824287415\n",
            "Training loss : 0.40337732434272766\n",
            "Training loss : 0.3874279260635376\n",
            "Training loss : 0.3650432825088501\n",
            "Training loss : 0.397054523229599\n",
            "Training loss : 0.40294572710990906\n",
            "Training loss : 0.41290757060050964\n",
            "Training loss : 0.37392789125442505\n",
            "Training loss : 0.34468305110931396\n",
            "Training loss : 0.40232348442077637\n",
            "Training loss : 0.45558062195777893\n",
            "Training loss : 0.3904060125350952\n",
            "Training loss : 0.4319532513618469\n",
            "Training loss : 0.38205695152282715\n",
            "Training loss : 0.36031749844551086\n",
            "===========================================================\n",
            "Test Acc on Epoch 23: 0.6746\n",
            "===========================================================\n",
            "Training loss : 0.3156597316265106\n",
            "Training loss : 0.3322318196296692\n",
            "Training loss : 0.4056932032108307\n",
            "Training loss : 0.3596520721912384\n",
            "Training loss : 0.2979891300201416\n",
            "Training loss : 0.3701007068157196\n",
            "Training loss : 0.3184357285499573\n",
            "Training loss : 0.35034120082855225\n",
            "Training loss : 0.37127161026000977\n",
            "Training loss : 0.3408583402633667\n",
            "Training loss : 0.31455814838409424\n",
            "Training loss : 0.3419080376625061\n",
            "Training loss : 0.31153255701065063\n",
            "Training loss : 0.3585651218891144\n",
            "Training loss : 0.351313978433609\n",
            "Training loss : 0.3174418807029724\n",
            "Training loss : 0.38492295145988464\n",
            "Training loss : 0.364170640707016\n",
            "Training loss : 0.3354591131210327\n",
            "Training loss : 0.36770933866500854\n",
            "Training loss : 0.3349246084690094\n",
            "Training loss : 0.37997809052467346\n",
            "Training loss : 0.3605715334415436\n",
            "Training loss : 0.3674401342868805\n",
            "Training loss : 0.33334994316101074\n",
            "Training loss : 0.3492162525653839\n",
            "Training loss : 0.3288447856903076\n",
            "Training loss : 0.34002307057380676\n",
            "Training loss : 0.3611757755279541\n",
            "Training loss : 0.328602135181427\n",
            "Training loss : 0.36392614245414734\n",
            "Training loss : 0.3603214621543884\n",
            "Training loss : 0.36923959851264954\n",
            "Training loss : 0.3338404893875122\n",
            "Training loss : 0.3477909564971924\n",
            "Training loss : 0.3821418285369873\n",
            "Training loss : 0.3412322998046875\n",
            "Training loss : 0.37097644805908203\n",
            "Training loss : 0.38648077845573425\n",
            "Training loss : 0.37735483050346375\n",
            "===========================================================\n",
            "Test Acc on Epoch 24: 0.6792\n",
            "===========================================================\n",
            "Training loss : 0.32719478011131287\n",
            "Training loss : 0.33000510931015015\n",
            "Training loss : 0.3317849040031433\n",
            "Training loss : 0.2990592122077942\n",
            "Training loss : 0.3192065954208374\n",
            "Training loss : 0.30207905173301697\n",
            "Training loss : 0.32637813687324524\n",
            "Training loss : 0.3179132044315338\n",
            "Training loss : 0.31090420484542847\n",
            "Training loss : 0.3075966238975525\n",
            "Training loss : 0.31788066029548645\n",
            "Training loss : 0.2939590811729431\n",
            "Training loss : 0.3372632563114166\n",
            "Training loss : 0.3349672257900238\n",
            "Training loss : 0.3423656225204468\n",
            "Training loss : 0.3504282534122467\n",
            "Training loss : 0.3406142592430115\n",
            "Training loss : 0.31393760442733765\n",
            "Training loss : 0.30960774421691895\n",
            "Training loss : 0.35073089599609375\n",
            "Training loss : 0.290227472782135\n",
            "Training loss : 0.3945882022380829\n",
            "Training loss : 0.33225342631340027\n",
            "Training loss : 0.36903777718544006\n",
            "Training loss : 0.31000635027885437\n",
            "Training loss : 0.3889940083026886\n",
            "Training loss : 0.3266621530056\n",
            "Training loss : 0.31843599677085876\n",
            "Training loss : 0.3263188898563385\n",
            "Training loss : 0.3610895872116089\n",
            "Training loss : 0.31946325302124023\n",
            "Training loss : 0.3878558874130249\n",
            "Training loss : 0.36874815821647644\n",
            "Training loss : 0.3432135581970215\n",
            "Training loss : 0.34234619140625\n",
            "Training loss : 0.37446606159210205\n",
            "Training loss : 0.34496912360191345\n",
            "Training loss : 0.3491518795490265\n",
            "Training loss : 0.33658838272094727\n",
            "Training loss : 0.36717841029167175\n",
            "===========================================================\n",
            "Test Acc on Epoch 25: 0.6744\n",
            "===========================================================\n",
            "Training loss : 0.2975616455078125\n",
            "Training loss : 0.319308876991272\n",
            "Training loss : 0.32629695534706116\n",
            "Training loss : 0.2913958728313446\n",
            "Training loss : 0.2849213182926178\n",
            "Training loss : 0.29086026549339294\n",
            "Training loss : 0.3306986689567566\n",
            "Training loss : 0.3113212287425995\n",
            "Training loss : 0.32798779010772705\n",
            "Training loss : 0.35205864906311035\n",
            "Training loss : 0.3607957661151886\n",
            "Training loss : 0.2841109037399292\n",
            "Training loss : 0.3501887023448944\n",
            "Training loss : 0.3072947561740875\n",
            "Training loss : 0.2957555651664734\n",
            "Training loss : 0.36193591356277466\n",
            "Training loss : 0.31958645582199097\n",
            "Training loss : 0.3522564768791199\n",
            "Training loss : 0.3245052993297577\n",
            "Training loss : 0.3079991340637207\n",
            "Training loss : 0.2687099575996399\n",
            "Training loss : 0.35637447237968445\n",
            "Training loss : 0.3244178891181946\n",
            "Training loss : 0.2861040234565735\n",
            "Training loss : 0.3032740354537964\n",
            "Training loss : 0.32376664876937866\n",
            "Training loss : 0.2966460883617401\n",
            "Training loss : 0.32159435749053955\n",
            "Training loss : 0.3102291524410248\n",
            "Training loss : 0.29781562089920044\n",
            "Training loss : 0.32981041073799133\n",
            "Training loss : 0.3385299742221832\n",
            "Training loss : 0.32846319675445557\n",
            "Training loss : 0.33749231696128845\n",
            "Training loss : 0.31473520398139954\n",
            "Training loss : 0.3147881031036377\n",
            "Training loss : 0.3041892945766449\n",
            "Training loss : 0.334563672542572\n",
            "Training loss : 0.3546244204044342\n",
            "Training loss : 0.3114739954471588\n",
            "===========================================================\n",
            "Test Acc on Epoch 26: 0.6838\n",
            "===========================================================\n",
            "Training loss : 0.3043101727962494\n",
            "Training loss : 0.28100863099098206\n",
            "Training loss : 0.2945204973220825\n",
            "Training loss : 0.2442500740289688\n",
            "Training loss : 0.29546743631362915\n",
            "Training loss : 0.2901109755039215\n",
            "Training loss : 0.30148690938949585\n",
            "Training loss : 0.2558844983577728\n",
            "Training loss : 0.296215295791626\n",
            "Training loss : 0.3282650113105774\n",
            "Training loss : 0.2683826684951782\n",
            "Training loss : 0.26723334193229675\n",
            "Training loss : 0.3117818832397461\n",
            "Training loss : 0.2591434121131897\n",
            "Training loss : 0.32892897725105286\n",
            "Training loss : 0.3071879744529724\n",
            "Training loss : 0.2852846086025238\n",
            "Training loss : 0.33295947313308716\n",
            "Training loss : 0.31409594416618347\n",
            "Training loss : 0.30560770630836487\n",
            "Training loss : 0.32012268900871277\n",
            "Training loss : 0.3233664929866791\n",
            "Training loss : 0.27482450008392334\n",
            "Training loss : 0.27501410245895386\n",
            "Training loss : 0.29675552248954773\n",
            "Training loss : 0.31939128041267395\n",
            "Training loss : 0.3279149830341339\n",
            "Training loss : 0.29584801197052\n",
            "Training loss : 0.3707239329814911\n",
            "Training loss : 0.2889268696308136\n",
            "Training loss : 0.37272554636001587\n",
            "Training loss : 0.28137630224227905\n",
            "Training loss : 0.29548412561416626\n",
            "Training loss : 0.35819005966186523\n",
            "Training loss : 0.3185266852378845\n",
            "Training loss : 0.3007512390613556\n",
            "Training loss : 0.3002397119998932\n",
            "Training loss : 0.3200546205043793\n",
            "Training loss : 0.3081684708595276\n",
            "Training loss : 0.3151730000972748\n",
            "===========================================================\n",
            "Test Acc on Epoch 27: 0.6844\n",
            "===========================================================\n",
            "Training loss : 0.3023267686367035\n",
            "Training loss : 0.2664605677127838\n",
            "Training loss : 0.2524958848953247\n",
            "Training loss : 0.26523882150650024\n",
            "Training loss : 0.2548994719982147\n",
            "Training loss : 0.28738144040107727\n",
            "Training loss : 0.27753886580467224\n",
            "Training loss : 0.2803223431110382\n",
            "Training loss : 0.26951777935028076\n",
            "Training loss : 0.2771119475364685\n",
            "Training loss : 0.2659075856208801\n",
            "Training loss : 0.21978126466274261\n",
            "Training loss : 0.28939318656921387\n",
            "Training loss : 0.2725679874420166\n",
            "Training loss : 0.29251575469970703\n",
            "Training loss : 0.29625922441482544\n",
            "Training loss : 0.2973116338253021\n",
            "Training loss : 0.23537257313728333\n",
            "Training loss : 0.26558932662010193\n",
            "Training loss : 0.2688356637954712\n",
            "Training loss : 0.27558764815330505\n",
            "Training loss : 0.26579827070236206\n",
            "Training loss : 0.29238685965538025\n",
            "Training loss : 0.31045228242874146\n",
            "Training loss : 0.3558516204357147\n",
            "Training loss : 0.3259383738040924\n",
            "Training loss : 0.28235772252082825\n",
            "Training loss : 0.30181196331977844\n",
            "Training loss : 0.3204621374607086\n",
            "Training loss : 0.29354822635650635\n",
            "Training loss : 0.31660810112953186\n",
            "Training loss : 0.29182636737823486\n",
            "Training loss : 0.3375641107559204\n",
            "Training loss : 0.2670937776565552\n",
            "Training loss : 0.2918638586997986\n",
            "Training loss : 0.30127736926078796\n",
            "Training loss : 0.27218517661094666\n",
            "Training loss : 0.31406867504119873\n",
            "Training loss : 0.25938335061073303\n",
            "Training loss : 0.35375890135765076\n",
            "===========================================================\n",
            "Test Acc on Epoch 28: 0.6884\n",
            "===========================================================\n",
            "Training loss : 0.26859748363494873\n",
            "Training loss : 0.2797536551952362\n",
            "Training loss : 0.23082992434501648\n",
            "Training loss : 0.22734548151493073\n",
            "Training loss : 0.26837044954299927\n",
            "Training loss : 0.22696362435817719\n",
            "Training loss : 0.2411314696073532\n",
            "Training loss : 0.2752832770347595\n",
            "Training loss : 0.24263016879558563\n",
            "Training loss : 0.19911299645900726\n",
            "Training loss : 0.21682003140449524\n",
            "Training loss : 0.26083651185035706\n",
            "Training loss : 0.29794707894325256\n",
            "Training loss : 0.2632889449596405\n",
            "Training loss : 0.2673831284046173\n",
            "Training loss : 0.2978878319263458\n",
            "Training loss : 0.3000018000602722\n",
            "Training loss : 0.28876614570617676\n",
            "Training loss : 0.28746771812438965\n",
            "Training loss : 0.25209739804267883\n",
            "Training loss : 0.3089510202407837\n",
            "Training loss : 0.31069236993789673\n",
            "Training loss : 0.27805575728416443\n",
            "Training loss : 0.29035624861717224\n",
            "Training loss : 0.2548058331012726\n",
            "Training loss : 0.2539203464984894\n",
            "Training loss : 0.3094048500061035\n",
            "Training loss : 0.31293559074401855\n",
            "Training loss : 0.3612580895423889\n",
            "Training loss : 0.37569648027420044\n",
            "Training loss : 0.2842068374156952\n",
            "Training loss : 0.274738609790802\n",
            "Training loss : 0.38051021099090576\n",
            "Training loss : 0.34107428789138794\n",
            "Training loss : 0.2703603506088257\n",
            "Training loss : 0.3962513208389282\n",
            "Training loss : 0.373419851064682\n",
            "Training loss : 0.3485433757305145\n",
            "Training loss : 0.33784639835357666\n",
            "Training loss : 0.31364724040031433\n",
            "===========================================================\n",
            "Test Acc on Epoch 29: 0.697\n",
            "===========================================================\n",
            "Training loss : 0.21653585135936737\n",
            "Training loss : 0.2597561478614807\n",
            "Training loss : 0.272091805934906\n",
            "Training loss : 0.27732571959495544\n",
            "Training loss : 0.24819476902484894\n",
            "Training loss : 0.275243878364563\n",
            "Training loss : 0.25955337285995483\n",
            "Training loss : 0.2488509565591812\n",
            "Training loss : 0.2663463354110718\n",
            "Training loss : 0.2875480353832245\n",
            "Training loss : 0.21739430725574493\n",
            "Training loss : 0.28155383467674255\n",
            "Training loss : 0.312116801738739\n",
            "Training loss : 0.23320671916007996\n",
            "Training loss : 0.2424239218235016\n",
            "Training loss : 0.3032340109348297\n",
            "Training loss : 0.25978097319602966\n",
            "Training loss : 0.26978564262390137\n",
            "Training loss : 0.23361580073833466\n",
            "Training loss : 0.2709781527519226\n",
            "Training loss : 0.25509345531463623\n",
            "Training loss : 0.24395452439785004\n",
            "Training loss : 0.22424949705600739\n",
            "Training loss : 0.20744608342647552\n",
            "Training loss : 0.2090572565793991\n",
            "Training loss : 0.23819294571876526\n",
            "Training loss : 0.2962760627269745\n",
            "Training loss : 0.26982995867729187\n",
            "Training loss : 0.2716820240020752\n",
            "Training loss : 0.2274097502231598\n",
            "Training loss : 0.291929692029953\n",
            "Training loss : 0.23493239283561707\n",
            "Training loss : 0.3007742166519165\n",
            "Training loss : 0.28440895676612854\n",
            "Training loss : 0.26357629895210266\n",
            "Training loss : 0.2832255959510803\n",
            "Training loss : 0.2774195969104767\n",
            "Training loss : 0.27367207407951355\n",
            "Training loss : 0.27783167362213135\n",
            "Training loss : 0.23816487193107605\n",
            "===========================================================\n",
            "Test Acc on Epoch 30: 0.6808\n",
            "===========================================================\n",
            "Training loss : 0.23626640439033508\n",
            "Training loss : 0.2691652178764343\n",
            "Training loss : 0.2263430505990982\n",
            "Training loss : 0.3094046413898468\n",
            "Training loss : 0.2945616543292999\n",
            "Training loss : 0.25610002875328064\n",
            "Training loss : 0.3469102382659912\n",
            "Training loss : 0.2512560486793518\n",
            "Training loss : 0.2911508083343506\n",
            "Training loss : 0.2506386935710907\n",
            "Training loss : 0.23369872570037842\n",
            "Training loss : 0.2885057330131531\n",
            "Training loss : 0.3934275507926941\n",
            "Training loss : 0.24662905931472778\n",
            "Training loss : 0.24896523356437683\n",
            "Training loss : 0.30365535616874695\n",
            "Training loss : 0.260865181684494\n",
            "Training loss : 0.30036821961402893\n",
            "Training loss : 0.24844232201576233\n",
            "Training loss : 0.24786417186260223\n",
            "Training loss : 0.275312215089798\n",
            "Training loss : 0.24927465617656708\n",
            "Training loss : 0.2775343656539917\n",
            "Training loss : 0.24370932579040527\n",
            "Training loss : 0.236892431974411\n",
            "Training loss : 0.24535569548606873\n",
            "Training loss : 0.2439633160829544\n",
            "Training loss : 0.2572685480117798\n",
            "Training loss : 0.2857198715209961\n",
            "Training loss : 0.25968292355537415\n",
            "Training loss : 0.2583182454109192\n",
            "Training loss : 0.22253389656543732\n",
            "Training loss : 0.23038774728775024\n",
            "Training loss : 0.2430846244096756\n",
            "Training loss : 0.2813103199005127\n",
            "Training loss : 0.28967559337615967\n",
            "Training loss : 0.21992133557796478\n",
            "Training loss : 0.2563772201538086\n",
            "Training loss : 0.26076656579971313\n",
            "Training loss : 0.24159082770347595\n",
            "===========================================================\n",
            "Test Acc on Epoch 31: 0.6902\n",
            "===========================================================\n",
            "Training loss : 0.185494065284729\n",
            "Training loss : 0.1855420470237732\n",
            "Training loss : 0.25265103578567505\n",
            "Training loss : 0.21653011441230774\n",
            "Training loss : 0.20649588108062744\n",
            "Training loss : 0.21062295138835907\n",
            "Training loss : 0.2113625407218933\n",
            "Training loss : 0.18026097118854523\n",
            "Training loss : 0.2525392770767212\n",
            "Training loss : 0.2516346275806427\n",
            "Training loss : 0.26546329259872437\n",
            "Training loss : 0.2002354860305786\n",
            "Training loss : 0.2247670292854309\n",
            "Training loss : 0.2380877435207367\n",
            "Training loss : 0.23787441849708557\n",
            "Training loss : 0.22081346809864044\n",
            "Training loss : 0.190003901720047\n",
            "Training loss : 0.22266918420791626\n",
            "Training loss : 0.23161648213863373\n",
            "Training loss : 0.1959594190120697\n",
            "Training loss : 0.23929715156555176\n",
            "Training loss : 0.2124502807855606\n",
            "Training loss : 0.43742284178733826\n",
            "Training loss : 0.46818381547927856\n",
            "Training loss : 0.5062817335128784\n",
            "Training loss : 0.5345951914787292\n",
            "Training loss : 0.5873478055000305\n",
            "Training loss : 0.49527063965797424\n",
            "Training loss : 0.6238687038421631\n",
            "Training loss : 0.5688366889953613\n",
            "Training loss : 0.5372350811958313\n",
            "Training loss : 0.5418002605438232\n",
            "Training loss : 0.5472872853279114\n",
            "Training loss : 0.525232195854187\n",
            "Training loss : 0.5233626961708069\n",
            "Training loss : 0.47308510541915894\n",
            "Training loss : 0.547184407711029\n",
            "Training loss : 0.5658761858940125\n",
            "Training loss : 0.4998999834060669\n",
            "Training loss : 0.44117259979248047\n",
            "===========================================================\n",
            "Test Acc on Epoch 32: 0.6048\n",
            "===========================================================\n",
            "Training loss : 0.3713688552379608\n",
            "Training loss : 0.36700159311294556\n",
            "Training loss : 0.3250752091407776\n",
            "Training loss : 0.3040681481361389\n",
            "Training loss : 0.3600125014781952\n",
            "Training loss : 0.36575567722320557\n",
            "Training loss : 0.3459348976612091\n",
            "Training loss : 0.4038190245628357\n",
            "Training loss : 0.35897377133369446\n",
            "Training loss : 0.3288060426712036\n",
            "Training loss : 0.33040252327919006\n",
            "Training loss : 0.33454588055610657\n",
            "Training loss : 0.29796308279037476\n",
            "Training loss : 0.3147023320198059\n",
            "Training loss : 0.3378240764141083\n",
            "Training loss : 0.2947847247123718\n",
            "Training loss : 0.3111630082130432\n",
            "Training loss : 0.30461448431015015\n",
            "Training loss : 0.31650418043136597\n",
            "Training loss : 0.2843114137649536\n",
            "Training loss : 0.28773632645606995\n",
            "Training loss : 0.3133629560470581\n",
            "Training loss : 0.3243044316768646\n",
            "Training loss : 0.3346119821071625\n",
            "Training loss : 0.31111058592796326\n",
            "Training loss : 0.258226215839386\n",
            "Training loss : 0.2567077577114105\n",
            "Training loss : 0.29225870966911316\n",
            "Training loss : 0.2997622787952423\n",
            "Training loss : 0.2627313733100891\n",
            "Training loss : 0.3052937984466553\n",
            "Training loss : 0.28260910511016846\n",
            "Training loss : 0.2915283739566803\n",
            "Training loss : 0.22363486886024475\n",
            "Training loss : 0.2973856031894684\n",
            "Training loss : 0.2771018147468567\n",
            "Training loss : 0.24389132857322693\n",
            "Training loss : 0.25516802072525024\n",
            "Training loss : 0.2836048901081085\n",
            "Training loss : 0.27685341238975525\n",
            "===========================================================\n",
            "Test Acc on Epoch 33: 0.6566\n",
            "===========================================================\n",
            "Training loss : 0.24421954154968262\n",
            "Training loss : 0.22544081509113312\n",
            "Training loss : 0.20986664295196533\n",
            "Training loss : 0.2812346816062927\n",
            "Training loss : 0.26315823197364807\n",
            "Training loss : 0.23144406080245972\n",
            "Training loss : 0.23927293717861176\n",
            "Training loss : 0.24567951261997223\n",
            "Training loss : 0.19058004021644592\n",
            "Training loss : 0.22306795418262482\n",
            "Training loss : 0.20663954317569733\n",
            "Training loss : 0.19232922792434692\n",
            "Training loss : 0.2592982053756714\n",
            "Training loss : 0.21086753904819489\n",
            "Training loss : 0.23413115739822388\n",
            "Training loss : 0.22891022264957428\n",
            "Training loss : 0.21842975914478302\n",
            "Training loss : 0.2366640716791153\n",
            "Training loss : 0.23300375044345856\n",
            "Training loss : 0.22588659822940826\n",
            "Training loss : 0.22244761884212494\n",
            "Training loss : 0.21058624982833862\n",
            "Training loss : 0.2401656210422516\n",
            "Training loss : 0.21630972623825073\n",
            "Training loss : 0.24408967792987823\n",
            "Training loss : 0.2229575365781784\n",
            "Training loss : 0.2160877138376236\n",
            "Training loss : 0.23839400708675385\n",
            "Training loss : 0.24225889146327972\n",
            "Training loss : 0.2119782418012619\n",
            "Training loss : 0.25865209102630615\n",
            "Training loss : 0.1820628046989441\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f159d03837b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-7afba0f2d514>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 269\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "===========================================================\n",
        "Test Acc on Epoch 13: 0.6954\n",
        "==========================================================="
      ],
      "metadata": {
        "id": "cwi8nlhYdgwe"
      }
    }
  ]
}